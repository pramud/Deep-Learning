{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#reading the MNIST data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Without any hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 accuracy:  0.1963\n",
      "epoch:  50 accuracy:  0.8209\n",
      "epoch:  100 accuracy:  0.8577\n",
      "epoch:  150 accuracy:  0.8709\n",
      "epoch:  200 accuracy:  0.8799\n",
      "epoch:  250 accuracy:  0.8854\n"
     ]
    }
   ],
   "source": [
    "#building a simple logistic regression using tensorflow\n",
    "n_features = len(mnist.train.images[0])\n",
    "n_labels = len(mnist.train.labels[0])\n",
    "train_features = mnist.train.images\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "validation_features = mnist.\n",
    "test_features = mnist.test.images\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "#for feeding data into neural network we use placeholder\n",
    "features = tf.placeholder(tf.float32,shape=[None,n_features])\n",
    "y = tf.placeholder(tf.float32,shape=[None,n_labels])\n",
    "#Weights are variables which are learnt during training of a neural network\n",
    "w = tf.Variable(tf.truncated_normal((n_features,n_labels)))\n",
    "b = tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "learning_rate = 0.005\n",
    "#with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #output = wx+b\n",
    "    y_ = tf.add(tf.matmul(features,w),b)\n",
    "    #for calculating error cross entropy\n",
    "    #cross_entropy = tf.reduce_sum(tf.multiply(y,tf.log(y_)),reduction_indices=1)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y)\n",
    "    loss = tf.reduce_mean(cross_entropy)    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    prediction = tf.nn.softmax(y_)\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    for epoch in range(300):\n",
    "        for f,l in batches(128,train_features,train_labels):\n",
    "            sess.run([optimizer],feed_dict={features:f,y:l})\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={features: test_features, y: test_labels})\n",
    "        if epoch%50==0:\n",
    "            print(\"epoch: \",epoch,\"accuracy: \",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89000005"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Network With one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 accuracy:  0.7261\n",
      "epoch:  50 accuracy:  0.9157\n",
      "epoch:  100 accuracy:  0.9263\n",
      "epoch:  150 accuracy:  0.9303\n",
      "epoch:  200 accuracy:  0.9344\n",
      "epoch:  250 accuracy:  0.9365\n"
     ]
    }
   ],
   "source": [
    "n_hunits = 256\n",
    "wh = tf.Variable(tf.truncated_normal((n_features,n_hunits)))\n",
    "bh = tf.Variable(tf.zeros(n_hunits))\n",
    "h_layer = tf.add(tf.matmul(features,wh),bh)\n",
    "h_layer = tf.nn.relu(h_layer)\n",
    "\n",
    "wo = tf.Variable(tf.truncated_normal((n_hunits,n_labels)))\n",
    "bo = tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "#output = wx+b\n",
    "y_ = tf.add(tf.matmul(h_layer,wo),bo)\n",
    "\n",
    "learning_rate = 0.005\n",
    "#with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #for calculating error cross entropy\n",
    "    #cross_entropy = tf.reduce_sum(tf.multiply(y,tf.log(y_)),reduction_indices=1)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y)\n",
    "    loss = tf.reduce_mean(cross_entropy)    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    prediction = tf.nn.softmax(y_)\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    for epoch in range(300):\n",
    "        for f,l in batches(128,train_features,train_labels):\n",
    "            sess.run([optimizer],feed_dict={features:f,y:l})\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={features: test_features, y: test_labels})\n",
    "        if epoch%50==0:\n",
    "            print(\"epoch: \",epoch,\"accuracy: \",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
